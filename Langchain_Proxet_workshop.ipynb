{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RRYSu48huSUW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7072f92-e45d-4ded-df3c-02cd38b383f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install langchain huggingface_hub openai google-search-results tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing and Evaluating LLMs"
      ],
      "metadata": {
        "id": "e--hMIfWIwsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "dNA4TsHpu6OM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show langchain"
      ],
      "metadata": {
        "id": "J-KFB7J_u_3L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2685b0cc-9809-4f0d-e328-c9de35f3b459"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: langchain\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the LLMs"
      ],
      "metadata": {
        "id": "qjtcjrq7PnAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "overal_temperature = 0.1"
      ],
      "metadata": {
        "id": "3arHx18MQqIb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up Flan models\n"
      ],
      "metadata": {
        "id": "HqwsGJDhvAQ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "\n",
        "flan_20B = HuggingFaceHub(repo_id=\"google/flan-ul2\",\n",
        "                         model_kwargs={\"temperature\":overal_temperature,\n",
        "                                       \"max_new_tokens\":200}\n",
        "                         )"
      ],
      "metadata": {
        "id": "lgesD0jrvDyG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flan_t5xxl = HuggingFaceHub(repo_id=\"google/flan-t5-xxl\",\n",
        "                         model_kwargs={\"temperature\":overal_temperature,\n",
        "                                       \"max_new_tokens\":200}\n",
        "                         )"
      ],
      "metadata": {
        "id": "ys9FQLsISSCK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# roberta = HuggingFaceHub(repo_id=\"TheBloke/Falcon-180B-Chat-GGUF\",\n",
        "#                          model_kwargs={\"temperature\":overal_temperature,\n",
        "#                                        \"max_new_tokens\":200}\n",
        "#                          )\n"
      ],
      "metadata": {
        "id": "bsKbbBgaT8MX"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unfortunately not working\n",
        "# GPTNeoXT_20B = HuggingFaceHub(repo_id=\"togethercomputer/GPT-NeoXT-Chat-Base-20B\",\n",
        "#                          model_kwargs={\"temperature\":overal_temperature,\n",
        "#                                        \"max_new_tokens\":200}\n",
        "#                          ) #bigscience/bloom-7b1"
      ],
      "metadata": {
        "id": "02EPvATsQytC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unfortunately not working\n",
        "# bloom7B = HuggingFaceHub(repo_id=\"bigscience/bloom-7b1\",\n",
        "#                          model_kwargs={\"temperature\":overal_temperature,\n",
        "#                                        \"max_new_tokens\":200}\n",
        "#                          )\n",
        "\n",
        "# gpt_j6B = HuggingFaceHub(repo_id=\"EleutherAI/gpt-j-6B\",\n",
        "#                          model_kwargs={\"temperature\":overal_temperature,\n",
        "#                                        \"max_new_tokens\":100}\n",
        "#                          )"
      ],
      "metadata": {
        "id": "HgVv5srjXZOK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setting up OpenAI models"
      ],
      "metadata": {
        "id": "M6yiwXNnvzxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI, OpenAIChat\n",
        "\n",
        "chatGPT_turbo = OpenAIChat(model_name='gpt-3.5-turbo-16k',\n",
        "             temperature=overal_temperature,\n",
        "             max_tokens = 256,\n",
        "             )\n",
        "\n",
        "# gpt3_davinici_003 = OpenAI(model_name='text-davinci-003',\n",
        "#              temperature=overal_temperature,\n",
        "#              max_tokens = 256,\n",
        "#              )"
      ],
      "metadata": {
        "id": "-lzO5PfUpwfv",
        "outputId": "2b41ea16-8619-41df-bee9-97d0b78852ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:787: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up a comparison lab"
      ],
      "metadata": {
        "id": "MDWW8nDERcGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.model_laboratory import ModelLaboratory"
      ],
      "metadata": {
        "id": "sy4s37W9m7X6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])"
      ],
      "metadata": {
        "id": "zZUMGKuvn_HV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_list = [\n",
        "    flan_20B,\n",
        "    flan_t5xxl,\n",
        "    chatGPT_turbo\n",
        "]"
      ],
      "metadata": {
        "id": "N5jHtwgkabw3"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "9s07AIuJm_Gv"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run it on some and compare!"
      ],
      "metadata": {
        "id": "LDQ8VcedrkIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare(\"What is the opposite of up?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Eb6pNpimfu6",
        "outputId": "0a3b61da-4966-4565-c57e-f484131b3882"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "What is the opposite of up?\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mDown is the opposite of up. Up is a direction. Down is a location. The answer: down.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mDown is the opposite of up. Down is the direction of a downward motion. The answer: down.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mThe opposite of up is down.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare(\"Answer the following question by reasoning step by step. The cafeteria had 23 apples. \\\n",
        "If they used 20 for lunch, and bought 6 more, how many apple do they have?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puWRd2nwT5eD",
        "outputId": "c980cb62-6873-4048-f735-bad88d835edd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mThey had 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. So the answer is 9.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mThe cafeteria had 23 - 20 = 3 apples after lunch. They bought 6 + 3 = 7 apples. The cafeteria has 7 - 3 = 2 apples.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mStep 1: The cafeteria had 23 apples.\n",
            "Step 2: They used 20 apples for lunch.\n",
            "Step 3: Subtracting the apples used for lunch from the total number of apples, we get 23 - 20 = 3 apples remaining.\n",
            "Step 4: The cafeteria bought 6 more apples.\n",
            "Step 5: Adding the apples bought to the remaining apples, we get 3 + 6 = 9 apples in total.\n",
            "Therefore, the cafeteria has 9 apples.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''\n",
        "Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP63DmDbaY_X",
        "outputId": "9242cf98-371a-4026-d16d-1ea3fbd2ae87"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "\n",
            "Can Elon Musk have a conversation with George Washington? Give the rationale before answering.\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mGeorge Washington died in 1799. Elon Musk was born in 1971. So, the final answer is no.\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mElon Musk is a billionaire entrepreneur who has developed a time machine. The time machine allows him to travel back in time and communicate with people who have died. George Washington died in 1799. So the final answer is no.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mFirst, we need to establish that George Washington is no longer alive. He passed away in 1799. Therefore, it is not possible for Elon Musk to have a direct conversation with him in the traditional sense.\n",
            "\n",
            "However, if we consider the possibility of time travel or some form of advanced technology that allows communication with individuals from the past, it is still highly unlikely that Elon Musk could have a conversation with George Washington.\n",
            "\n",
            "George Washington lived in a time when technology was significantly less advanced compared to the present day. The concepts, ideas, and knowledge that Elon Musk possesses would be far beyond what George Washington could comprehend or engage with.\n",
            "\n",
            "Additionally, the language and cultural differences between the two individuals would pose a significant barrier to effective communication. George Washington spoke English, but the language has evolved and changed over the centuries. Musk's modern vocabulary, slang, and references would likely be confusing and unfamiliar to Washington.\n",
            "\n",
            "Furthermore, the vast differences in their life experiences, societal contexts, and worldviews would make it challenging for them to find common ground or have meaningful discussions. Their perspectives on politics, science, technology, and society would be vastly different due to the vast historical and cultural gaps between them.\n",
            "\n",
            "In conclusion, while it may be theoretically possible to imagine a scenario where Elon Musk could have a\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's change the prompt."
      ],
      "metadata": {
        "id": "mbqm52kHarID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"You are a professional social media manager who can write great posts in linkedin to increase appeal of persons profile: {request}\n",
        "\n",
        "Story:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"request\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "Uoq5C6eMT5jr"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''I have passed a course in large language models (1 month duration). Write a post about that.''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lo1mo_8OT5nI",
        "outputId": "7ff949b1-c00f-47f2-a9f6-1f0ec72bfb61"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I have passed a course in large language models (1 month duration). Write a post about that.\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mI have passed a course in large language models (1 month duration)\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mI have passed a course in large language models (1 month duration)\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3m🎉 Exciting News! 🎉\n",
            "\n",
            "I am thrilled to announce that I have successfully completed a comprehensive course in large language models! 📚✨ Over the past month, I have delved deep into the world of cutting-edge technology and honed my skills in harnessing the power of these incredible models. 🌟\n",
            "\n",
            "During this course, I have gained a profound understanding of how large language models work and their immense potential in transforming various industries. From natural language processing to text generation, these models have the ability to revolutionize the way we communicate and interact with technology. 🚀💡\n",
            "\n",
            "Through hands-on projects and engaging discussions, I have not only learned how to effectively utilize these models but also discovered innovative ways to apply them in real-world scenarios. The course has equipped me with the knowledge and expertise to leverage large language models to enhance content creation, improve customer experiences, and drive meaningful engagement. 📝💬\n",
            "\n",
            "I am incredibly grateful for the opportunity to learn from industry experts and collaborate with fellow enthusiasts who share the same passion for this groundbreaking technology. The course has not only expanded my technical skills but also broadened my horizons, allowing me to stay at the forefront of the latest advancements in the field. 🌐\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Answer the question to the best of your abilities but if you are not sure then answer you don't know: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "c2c6EByDT5rQ"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?''')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCuIkZST5uK",
        "outputId": "3ae8c285-7a94-44c9-a987-eee8fa2b2758"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "I am riding a bicycle. The pedals are moving fast. I look into the mirror and I am not moving. Why is this?\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mI am stationary\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mI am looking at the wrong thing.\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mI don't know.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition\n",
        "\n"
      ],
      "metadata": {
        "id": "lOks9w5ndiqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"{question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "lab = ModelLaboratory.from_llms(models_list, prompt=prompt)"
      ],
      "metadata": {
        "id": "JydfVsh8eAFx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''Extract names and cities from the text.\\n\\n\n",
        "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
        "when doctors deemed him “transportable.”\n",
        "''')"
      ],
      "metadata": {
        "id": "z2jB3n87cYsj",
        "outputId": "a2ddaec6-8ebf-47db-b568-4205beab09f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Extract names and cities from the text.\n",
            "\n",
            "\n",
            "Output in the format: {\"names\": list of names in text, \"cities\": list of cities in text}, surname]\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday, when doctors deemed him “transportable.”\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mnames, surname]\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mnames[Mark Dickey, cities[Southern Turkey]\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3m{\"names\": [\"Mark Dickey\"], \"cities\": [\"Southern Turkey\"]}\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question answering based on text"
      ],
      "metadata": {
        "id": "WQjad_C-dgDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lab.compare('''Is Mark Dickey alive?\\n\\n\n",
        "Output in the format: Yes or No, facts that prove that.\\n\\n\n",
        "\n",
        "Mark Dickey, 40, began suffering from severe gastric pain last week after descending\n",
        "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement\n",
        "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday,\n",
        "when doctors deemed him “transportable.”\n",
        "''')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crsqwwmrdLQl",
        "outputId": "6d0b4761-b1bf-43ed-da64-7bda01d8a705"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1mInput:\u001b[0m\n",
            "Is Mark Dickey alive?\n",
            "\n",
            "\n",
            "Output in the format: Yes or No, facts that prove that.\n",
            "\n",
            "\n",
            "\n",
            "Mark Dickey, 40, began suffering from severe gastric pain last week after descending \n",
            "more than 3,600 feet into Morca Cave in Southern Turkey, according to a statement \n",
            "from the European Cave Rescue Association, but he wasn’t able to be rescued until yesterday, \n",
            "when doctors deemed him “transportable.”\n",
            "\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-ul2', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[36;1m\u001b[1;3mYes\u001b[0m\n",
            "\n",
            "\u001b[1mHuggingFaceHub\u001b[0m\n",
            "Params: {'repo_id': 'google/flan-t5-xxl', 'task': None, 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 200}}\n",
            "\u001b[33;1m\u001b[1;3mYes\u001b[0m\n",
            "\n",
            "\u001b[1mOpenAIChat\u001b[0m\n",
            "Params: {'model_name': 'gpt-3.5-turbo-16k', 'temperature': 0.1, 'max_tokens': 256}\n",
            "\u001b[38;5;200m\u001b[1;3mYes, Mark Dickey is alive. He was rescued from Morca Cave in Southern Turkey after suffering from severe gastric pain and deemed \"transportable\" by doctors.\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Lu5RbicdpVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# In case we just want to use one model"
      ],
      "metadata": {
        "id": "vpKnXTEmduvL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TTPogSG7dvq-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}